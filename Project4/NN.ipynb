{"cells":[{"cell_type":"markdown","metadata":{"id":"5MAitc5mOT31"},"source":["by Suman Kumar Bera\n","skbera.iitkgp21@gmail.com\n","for Tutorial of AMLBS on 04/09/23\n","\n","Problem taken from: https://www.javatpoint.com/pytorch-backpropagation-process-in-deep-neural-network"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":469,"status":"ok","timestamp":1693807601735,"user":{"displayName":"Suman Bera","userId":"13633309447925846459"},"user_tz":-330},"id":"6FnrPWn3GOZt","outputId":"4ec1398f-8c87-4b62-9ab4-52b6ff69efa4"},"outputs":[{"data":{"text/plain":["array([[0.75136507, 0.77292847]])"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["## NN\n","\n","# Import the necessary libraries\n","import numpy as np\n","\n","# define the class\n","class NN:\n","\n","  def __init__(self, input_layer_size = 2, hidden_layer_size = 2, output_layer_size = 2, lr = 0.5, epoches = 1000):  # define the constructor\n","    self.input_layer_size = input_layer_size\n","    self.hidden_layer_size = hidden_layer_size\n","    self.output_layer_size = output_layer_size\n","    self.lr = lr\n","    self.number_of_epoches = epoches\n","\n","    #initialize the weights\n","    #1. by random numbers\n","    # self.W1 = np.random.randn(self.input_layer_size, self.hidden_layer_size)\n","    # self.W2 = np.random.randn(self.hidden_layer_size, self.output_layer_size)\n","    # print(f'W1 = {self.W1}')\n","    # print(f'W2 = {self.W2}')\n","\n","    #2. with zeros\n","    # self.W1 = np.zeros((self.input_layer_size, self.hidden_layer_size))\n","    # self.W2 = np.zeros((self.hidden_layer_size, self.output_layer_size))\n","    # print(f'W1 = {self.W1}')\n","    # print(f'W2 = {self.W2}')\n","\n","    #3. specific numbers\n","    self.W1 = np.array([[0.15, 0.25], [0.20, 0.30]])  # ([[w1, w3], [w2, w4]])\n","    self.W2 = np.array([[0.40, 0.50], [0.45, 0.55]])\n","    # print(f'W1 = \\n{self.W1}')\n","    # print(f'W2 = \\n{self.W2}')\n","\n","    # bias\n","    self.b1 = 0.35\n","    self.b2 = 0.60\n","\n","  # define the activation function\n","  def activation(self, x):\n","    return (1 / (1 + np.exp(-x)))\n","\n","  # forward pass\n","  def forward(self, X):\n","    self.hidden = self.activation(np.dot(X, self.W1) + self.b1)  # H = activation(X*W1 + b)\n","    self.output = self.activation(np.dot(self.hidden, self.W2) + self.b2)  # OP = activation(H*W2 + b)\n","\n","    # print(f'H = {self.hidden}')\n","    # print(f'OP = {self.output}')\n","    return self.output\n","\n","  # back propagation\n","  def back_prop(self, X, y):\n","    output = self.forward(X)\n","    error = np.sum((y - output)**2/2)\n","    # print(f'Error = {error}')\n","    self.W2 -= self.lr * (np.dot(self.hidden.T, np.multiply(-(y - output), output * (1 - output)))) # W2 = W2 - alpha * (H * ((y-OP) * (OP * (OP - 1))))\n","    print(f'New W2 Weights: {self.W2}')\n","\n","    # self.W1 -= ?\n","\n","  # training; update the weights for the given number of epoches\n","  # def train(self, X, y):\n","  #   for _ in range(self.number_of_epoches):\n","  #     self.back_prop(X, y)\n","\n","\n","# create an instance of the class\n","model = NN()\n","# define the input and output\n","X = np.array([[0.05, 0.10]]) #input\n","y = np.array([[0.01, 0.99]]) #output\n","model.forward(X)\n","# model.back_prop(X, y)"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Solution with some updated code"]},{"cell_type":"markdown","metadata":{},"source":["Major updates:\n","1. Implemented Backpropagation\n","2. Implemented Scaling"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5-fold cross-validation results:\n","Case (a) - Training MSE: 15.57576277056953, Validation MSE: 15.572468716645304\n","Case (b) - Training MSE: 15.577722421522335, Validation MSE: 15.574440397281316\n","Case (c) - Training MSE: 15.598655449682076, Validation MSE: 15.595456408730433\n","\n","\n","10-fold cross-validation results:\n","Case (a) - Training MSE: 15.57592712368706, Validation MSE: 15.572950108195789\n","Case (b) - Training MSE: 15.577612566030677, Validation MSE: 15.574674562404576\n","Case (c) - Training MSE: 15.595804760749497, Validation MSE: 15.592842765086981\n"]}],"source":["# Importing necessary dependencies\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.preprocessing import StandardScaler\n","\n","# Modified Neural Network class to take user-defined input/output layers\n","class FlexibleNeuralNetwork:\n","    def __init__(self, input_size, hidden_size, output_size, learning_rate):\n","        \n","        # Initializing weights\n","        self.weights_input_hidden = np.random.rand(input_size, hidden_size) - 0.5\n","        self.weights_hidden_output = np.random.rand(hidden_size, output_size) - 0.5\n","        \n","        # Initalizing biases\n","        self.bias_hidden = np.random.rand(1, hidden_size) - 0.5\n","        self.bias_output = np.random.rand(1, output_size) - 0.5\n","        \n","        # Learning rate\n","        self.learning_rate = learning_rate\n","\n","    def sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))\n","    \n","    def sigmoid_derivative(self, x):\n","        return x * (1 - x)\n","    \n","    def forward(self, X):\n","        # Forward pass\n","        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n","        self.hidden_output = self.sigmoid(self.hidden_input)\n","        \n","        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n","        self.output = self.sigmoid(self.output_input)\n","        \n","        return self.output\n","    \n","    def backward(self, X, y, output):\n","        # Backward pass\n","        output_error = y - output\n","        output_delta = output_error * self.sigmoid_derivative(output)\n","        \n","        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n","        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n","        \n","        # Updating weights and biases\n","        self.weights_hidden_output += self.learning_rate * np.dot(self.hidden_output.T, output_delta)\n","        self.bias_output += self.learning_rate * np.sum(output_delta, axis=0, keepdims=True)\n","        \n","        self.weights_input_hidden += self.learning_rate * np.dot(X.T, hidden_delta)\n","        self.bias_hidden += self.learning_rate * np.sum(hidden_delta, axis=0, keepdims=True)\n","\n","    def train(self, X, y, epochs):\n","        for epoch in range(epochs):\n","            output = self.forward(X)\n","            self.backward(X, y, output)\n","\n","    def predict(self, X):\n","        return self.forward(X)\n","\n","\n","# Parameters for training\n","input_size = X_train.shape[1]\n","output_size = 1  # Single output for regression (house price)\n","epochs = 1000\n","\n","# Function to configure and train the model based on user input and calculate MSE for training and validation\n","def configure_and_train_nn(input_neurons, hidden_neurons, output_neurons, learning_rate, k_folds=5, epochs=1000):\n","    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n","    training_losses = []\n","    validation_losses = []\n","    fold_validation_losses = []  # This will store the validation losses for each fold\n","    \n","    for train_idx, val_idx in kfold.split(X_train):\n","        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n","        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n","        \n","        # Initializing the neural network with user-defined parameters\n","        nn = FlexibleNeuralNetwork(input_neurons, hidden_neurons, output_neurons, learning_rate)\n","        \n","        # Training the model\n","        nn.train(X_fold_train, y_fold_train, epochs)\n","        \n","        # Predicting on the training set\n","        train_predictions = nn.predict(X_fold_train)\n","        train_loss = np.mean((y_fold_train - train_predictions)**2)  # MSE for training set\n","        training_losses.append(train_loss)\n","        \n","        # Predicting on the validation set\n","        val_predictions = nn.predict(X_fold_val)\n","        val_loss = np.mean((y_fold_val - val_predictions)**2)  # MSE for validation set\n","        validation_losses.append(val_loss)\n","        fold_validation_losses.append(val_loss)\n","    \n","    # Calculating the average training and validation losses across all folds\n","    avg_train_loss = np.mean(training_losses)\n","    avg_val_loss = np.mean(validation_losses)\n","\n","    # Returning the original validation losses (fold-specific), average training and validation losses\n","    return fold_validation_losses, avg_train_loss, avg_val_loss\n","\n","\n","# Importing the data\n","housing_data = pd.read_csv('housing.csv')\n","\n","# Preparing data for training\n","X = housing_data[['RM', 'LSTAT', 'PTRATIO']].values\n","y = housing_data[['MEDV']].values / 100000  # Scaling the output target\n","\n","# Scaling the input features\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Splitting the data into training and validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=52)\n","\n","# Example of how the user might input parameters to configure the network\n","input_neurons = X_train.shape[1]  # 3 input features (RM, LSTAT, PTRATIO)\n","output_neurons = 1  # Single output for regression\n","\n","\n","# Parameters for case (a)\n","hidden_neurons_a = 3\n","learning_rate_a = 0.01\n","\n","# 5-fold cross-validation for case (a)\n","fold_val_losses_a_5folds, avg_train_loss_a_5folds, avg_val_loss_a_5folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_a, output_neurons, learning_rate_a)\n","\n","# 10-fold cross-validation for case (a)\n","fold_val_losses_a_10folds, avg_train_loss_a_10folds, avg_val_loss_a_10folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_a, output_neurons, learning_rate_a, k_folds=10)\n","\n","\n","# Parameters for case (b)\n","hidden_neurons_b = 4\n","learning_rate_b = 0.001\n","\n","# 5-fold cross-validation for case (b)\n","fold_val_losses_b_5folds, avg_train_loss_b_5folds, avg_val_loss_b_5folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_b, output_neurons, learning_rate_b)\n","\n","# 10-fold cross-validation for case (b)\n","fold_val_losses_b_10folds, avg_train_loss_b_10folds, avg_val_loss_b_10folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_b, output_neurons, learning_rate_b, k_folds=10)\n","\n","\n","# Parameters for case (c)\n","hidden_neurons_c = 5\n","learning_rate_c = 0.0001\n","\n","# 5-fold cross-validation for case (c)\n","fold_val_losses_c_5folds, avg_train_loss_c_5folds, avg_val_loss_c_5folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_c, output_neurons, learning_rate_c)\n","\n","# 10-fold cross-validation for case (c)\n","fold_val_losses_c_10folds, avg_train_loss_c_10folds, avg_val_loss_c_10folds = configure_and_train_nn(\n","    input_neurons, hidden_neurons_c, output_neurons, learning_rate_c, k_folds=10)\n","\n","\n","# Printing the 5-fold cross-validation results\n","print('5-fold cross-validation results:')\n","print(f'Case (a) - Training MSE: {avg_train_loss_a_5folds}, Validation MSE: {avg_val_loss_a_5folds}')\n","print(f'Case (b) - Training MSE: {avg_train_loss_b_5folds}, Validation MSE: {avg_val_loss_b_5folds}')\n","print(f'Case (c) - Training MSE: {avg_train_loss_c_5folds}, Validation MSE: {avg_val_loss_c_5folds}')\n","\n","print('\\n')\n","\n","# Printing the 10-fold cross-validation results\n","print('10-fold cross-validation results:')\n","print(f'Case (a) - Training MSE: {avg_train_loss_a_10folds}, Validation MSE: {avg_val_loss_a_10folds}')\n","print(f'Case (b) - Training MSE: {avg_train_loss_b_10folds}, Validation MSE: {avg_val_loss_b_10folds}')\n","print(f'Case (c) - Training MSE: {avg_train_loss_c_10folds}, Validation MSE: {avg_val_loss_c_10folds}')\n"]},{"cell_type":"markdown","metadata":{"vscode":{"languageId":"bat"}},"source":["# Potential reasons for not converging:\n","1. Inadequate learning rate (as asked in the problem statement)\n","2. Insufficient epochs\n","3. Network Structure (as asked in the problem statement)\n","4. Sigmoid activation function (as advised in the code above)\n","5. Poor weights initialization (as advised in the code above)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693735074929,"user":{"displayName":"Suman Bera","userId":"13633309447925846459"},"user_tz":-330},"id":"HEVd3utxcWpL","outputId":"f41f2040-fd41-4d9d-8a3f-a0fa7e19eba7"},"outputs":[{"name":"stdout","output_type":"stream","text":["np.dot = \n"," [[19 22]\n"," [43 50]]\n","np.multiply = \n"," [[ 5 12]\n"," [21 32]]\n"]}],"source":["# difference between np.dot and np.multiply\n","\n","a = np.array([[1, 2], [3, 4]])\n","b = np.array([[5, 6], [7, 8]])\n","\n","print(f'np.dot = \\n', np.dot(a, b))\n","print(f'np.multiply = \\n', np.multiply(a, b))"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1693740364748,"user":{"displayName":"Suman Bera","userId":"13633309447925846459"},"user_tz":-330},"id":"PFpt-FSMepSR","outputId":"0ac78286-5f23-4962-9e70-1adc36e630df"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>RM</th>\n","      <th>LSTAT</th>\n","      <th>PTRATIO</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6.575</td>\n","      <td>4.98</td>\n","      <td>15.3</td>\n","      <td>504000.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.421</td>\n","      <td>9.14</td>\n","      <td>17.8</td>\n","      <td>453600.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7.185</td>\n","      <td>4.03</td>\n","      <td>17.8</td>\n","      <td>728700.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6.998</td>\n","      <td>2.94</td>\n","      <td>18.7</td>\n","      <td>701400.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.147</td>\n","      <td>5.33</td>\n","      <td>18.7</td>\n","      <td>760200.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>484</th>\n","      <td>6.593</td>\n","      <td>9.67</td>\n","      <td>21.0</td>\n","      <td>470400.0</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>6.120</td>\n","      <td>9.08</td>\n","      <td>21.0</td>\n","      <td>432600.0</td>\n","    </tr>\n","    <tr>\n","      <th>486</th>\n","      <td>6.976</td>\n","      <td>5.64</td>\n","      <td>21.0</td>\n","      <td>501900.0</td>\n","    </tr>\n","    <tr>\n","      <th>487</th>\n","      <td>6.794</td>\n","      <td>6.48</td>\n","      <td>21.0</td>\n","      <td>462000.0</td>\n","    </tr>\n","    <tr>\n","      <th>488</th>\n","      <td>6.030</td>\n","      <td>7.88</td>\n","      <td>21.0</td>\n","      <td>249900.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>489 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["        RM  LSTAT  PTRATIO      MEDV\n","0    6.575   4.98     15.3  504000.0\n","1    6.421   9.14     17.8  453600.0\n","2    7.185   4.03     17.8  728700.0\n","3    6.998   2.94     18.7  701400.0\n","4    7.147   5.33     18.7  760200.0\n","..     ...    ...      ...       ...\n","484  6.593   9.67     21.0  470400.0\n","485  6.120   9.08     21.0  432600.0\n","486  6.976   5.64     21.0  501900.0\n","487  6.794   6.48     21.0  462000.0\n","488  6.030   7.88     21.0  249900.0\n","\n","[489 rows x 4 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Dataset\n","import pandas as pd\n","import numpy as np\n","\n","\n","df = pd.read_csv('housing.csv')\n","df"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1693740368768,"user":{"displayName":"Suman Bera","userId":"13633309447925846459"},"user_tz":-330},"id":"02YyMscugVye","outputId":"9bceeaa8-9582-47d4-b791-2fb5b94d2530"},"outputs":[{"name":"stdout","output_type":"stream","text":["(489, 3)\n","(489,)\n"]}],"source":["X_data = df.drop(columns=['MEDV'])\n","y_data = df['MEDV']\n","X_data = X_data.to_numpy()\n","y_data = y_data.to_numpy()\n","X_data = (X_data-np.min(X_data))/(np.max(X_data)-np.min(X_data))\n","y_data = (y_data-np.min(y_data))/(np.max(y_data)-np.min(y_data))\n","print(X_data.shape)\n","print(y_data.shape)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOWaaEpNfBbpWJ2/RCw6Rxt","mount_file_id":"1c8KUkHlHbuPn5EvcZ-hh_8ag5PeepmdY","provenance":[]},"kernelspec":{"display_name":"NLP_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
